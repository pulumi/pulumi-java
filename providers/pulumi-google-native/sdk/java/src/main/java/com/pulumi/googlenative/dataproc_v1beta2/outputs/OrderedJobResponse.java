// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.googlenative.dataproc_v1beta2.outputs;

import com.pulumi.core.annotations.CustomType;
import com.pulumi.googlenative.dataproc_v1beta2.outputs.HadoopJobResponse;
import com.pulumi.googlenative.dataproc_v1beta2.outputs.HiveJobResponse;
import com.pulumi.googlenative.dataproc_v1beta2.outputs.JobSchedulingResponse;
import com.pulumi.googlenative.dataproc_v1beta2.outputs.PigJobResponse;
import com.pulumi.googlenative.dataproc_v1beta2.outputs.PrestoJobResponse;
import com.pulumi.googlenative.dataproc_v1beta2.outputs.PySparkJobResponse;
import com.pulumi.googlenative.dataproc_v1beta2.outputs.SparkJobResponse;
import com.pulumi.googlenative.dataproc_v1beta2.outputs.SparkRJobResponse;
import com.pulumi.googlenative.dataproc_v1beta2.outputs.SparkSqlJobResponse;
import java.lang.String;
import java.util.List;
import java.util.Map;
import java.util.Objects;

@CustomType
public final class OrderedJobResponse {
    /**
     * @return Optional. Job is a Hadoop job.
     * 
     */
    private final HadoopJobResponse hadoopJob;
    /**
     * @return Optional. Job is a Hive job.
     * 
     */
    private final HiveJobResponse hiveJob;
    /**
     * @return Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
     * 
     */
    private final Map<String,String> labels;
    /**
     * @return Optional. Job is a Pig job.
     * 
     */
    private final PigJobResponse pigJob;
    /**
     * @return Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
     * 
     */
    private final List<String> prerequisiteStepIds;
    /**
     * @return Optional. Job is a Presto job.
     * 
     */
    private final PrestoJobResponse prestoJob;
    /**
     * @return Optional. Job is a PySpark job.
     * 
     */
    private final PySparkJobResponse pysparkJob;
    /**
     * @return Optional. Job scheduling configuration.
     * 
     */
    private final JobSchedulingResponse scheduling;
    /**
     * @return Optional. Job is a Spark job.
     * 
     */
    private final SparkJobResponse sparkJob;
    /**
     * @return Optional. Job is a SparkR job.
     * 
     */
    private final SparkRJobResponse sparkRJob;
    /**
     * @return Optional. Job is a SparkSql job.
     * 
     */
    private final SparkSqlJobResponse sparkSqlJob;
    /**
     * @return The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
     * 
     */
    private final String stepId;

    @CustomType.Constructor
    private OrderedJobResponse(
        @CustomType.Parameter("hadoopJob") HadoopJobResponse hadoopJob,
        @CustomType.Parameter("hiveJob") HiveJobResponse hiveJob,
        @CustomType.Parameter("labels") Map<String,String> labels,
        @CustomType.Parameter("pigJob") PigJobResponse pigJob,
        @CustomType.Parameter("prerequisiteStepIds") List<String> prerequisiteStepIds,
        @CustomType.Parameter("prestoJob") PrestoJobResponse prestoJob,
        @CustomType.Parameter("pysparkJob") PySparkJobResponse pysparkJob,
        @CustomType.Parameter("scheduling") JobSchedulingResponse scheduling,
        @CustomType.Parameter("sparkJob") SparkJobResponse sparkJob,
        @CustomType.Parameter("sparkRJob") SparkRJobResponse sparkRJob,
        @CustomType.Parameter("sparkSqlJob") SparkSqlJobResponse sparkSqlJob,
        @CustomType.Parameter("stepId") String stepId) {
        this.hadoopJob = hadoopJob;
        this.hiveJob = hiveJob;
        this.labels = labels;
        this.pigJob = pigJob;
        this.prerequisiteStepIds = prerequisiteStepIds;
        this.prestoJob = prestoJob;
        this.pysparkJob = pysparkJob;
        this.scheduling = scheduling;
        this.sparkJob = sparkJob;
        this.sparkRJob = sparkRJob;
        this.sparkSqlJob = sparkSqlJob;
        this.stepId = stepId;
    }

    /**
     * @return Optional. Job is a Hadoop job.
     * 
     */
    public HadoopJobResponse hadoopJob() {
        return this.hadoopJob;
    }
    /**
     * @return Optional. Job is a Hive job.
     * 
     */
    public HiveJobResponse hiveJob() {
        return this.hiveJob;
    }
    /**
     * @return Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \p{Ll}\p{Lo}\p{N}_-{0,63}No more than 32 labels can be associated with a given job.
     * 
     */
    public Map<String,String> labels() {
        return this.labels;
    }
    /**
     * @return Optional. Job is a Pig job.
     * 
     */
    public PigJobResponse pigJob() {
        return this.pigJob;
    }
    /**
     * @return Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
     * 
     */
    public List<String> prerequisiteStepIds() {
        return this.prerequisiteStepIds;
    }
    /**
     * @return Optional. Job is a Presto job.
     * 
     */
    public PrestoJobResponse prestoJob() {
        return this.prestoJob;
    }
    /**
     * @return Optional. Job is a PySpark job.
     * 
     */
    public PySparkJobResponse pysparkJob() {
        return this.pysparkJob;
    }
    /**
     * @return Optional. Job scheduling configuration.
     * 
     */
    public JobSchedulingResponse scheduling() {
        return this.scheduling;
    }
    /**
     * @return Optional. Job is a Spark job.
     * 
     */
    public SparkJobResponse sparkJob() {
        return this.sparkJob;
    }
    /**
     * @return Optional. Job is a SparkR job.
     * 
     */
    public SparkRJobResponse sparkRJob() {
        return this.sparkRJob;
    }
    /**
     * @return Optional. Job is a SparkSql job.
     * 
     */
    public SparkSqlJobResponse sparkSqlJob() {
        return this.sparkSqlJob;
    }
    /**
     * @return The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
     * 
     */
    public String stepId() {
        return this.stepId;
    }

    public static Builder builder() {
        return new Builder();
    }

    public static Builder builder(OrderedJobResponse defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private HadoopJobResponse hadoopJob;
        private HiveJobResponse hiveJob;
        private Map<String,String> labels;
        private PigJobResponse pigJob;
        private List<String> prerequisiteStepIds;
        private PrestoJobResponse prestoJob;
        private PySparkJobResponse pysparkJob;
        private JobSchedulingResponse scheduling;
        private SparkJobResponse sparkJob;
        private SparkRJobResponse sparkRJob;
        private SparkSqlJobResponse sparkSqlJob;
        private String stepId;

        public Builder() {
    	      // Empty
        }

        public Builder(OrderedJobResponse defaults) {
    	      Objects.requireNonNull(defaults);
    	      this.hadoopJob = defaults.hadoopJob;
    	      this.hiveJob = defaults.hiveJob;
    	      this.labels = defaults.labels;
    	      this.pigJob = defaults.pigJob;
    	      this.prerequisiteStepIds = defaults.prerequisiteStepIds;
    	      this.prestoJob = defaults.prestoJob;
    	      this.pysparkJob = defaults.pysparkJob;
    	      this.scheduling = defaults.scheduling;
    	      this.sparkJob = defaults.sparkJob;
    	      this.sparkRJob = defaults.sparkRJob;
    	      this.sparkSqlJob = defaults.sparkSqlJob;
    	      this.stepId = defaults.stepId;
        }

        public Builder hadoopJob(HadoopJobResponse hadoopJob) {
            this.hadoopJob = Objects.requireNonNull(hadoopJob);
            return this;
        }
        public Builder hiveJob(HiveJobResponse hiveJob) {
            this.hiveJob = Objects.requireNonNull(hiveJob);
            return this;
        }
        public Builder labels(Map<String,String> labels) {
            this.labels = Objects.requireNonNull(labels);
            return this;
        }
        public Builder pigJob(PigJobResponse pigJob) {
            this.pigJob = Objects.requireNonNull(pigJob);
            return this;
        }
        public Builder prerequisiteStepIds(List<String> prerequisiteStepIds) {
            this.prerequisiteStepIds = Objects.requireNonNull(prerequisiteStepIds);
            return this;
        }
        public Builder prerequisiteStepIds(String... prerequisiteStepIds) {
            return prerequisiteStepIds(List.of(prerequisiteStepIds));
        }
        public Builder prestoJob(PrestoJobResponse prestoJob) {
            this.prestoJob = Objects.requireNonNull(prestoJob);
            return this;
        }
        public Builder pysparkJob(PySparkJobResponse pysparkJob) {
            this.pysparkJob = Objects.requireNonNull(pysparkJob);
            return this;
        }
        public Builder scheduling(JobSchedulingResponse scheduling) {
            this.scheduling = Objects.requireNonNull(scheduling);
            return this;
        }
        public Builder sparkJob(SparkJobResponse sparkJob) {
            this.sparkJob = Objects.requireNonNull(sparkJob);
            return this;
        }
        public Builder sparkRJob(SparkRJobResponse sparkRJob) {
            this.sparkRJob = Objects.requireNonNull(sparkRJob);
            return this;
        }
        public Builder sparkSqlJob(SparkSqlJobResponse sparkSqlJob) {
            this.sparkSqlJob = Objects.requireNonNull(sparkSqlJob);
            return this;
        }
        public Builder stepId(String stepId) {
            this.stepId = Objects.requireNonNull(stepId);
            return this;
        }        public OrderedJobResponse build() {
            return new OrderedJobResponse(hadoopJob, hiveJob, labels, pigJob, prerequisiteStepIds, prestoJob, pysparkJob, scheduling, sparkJob, sparkRJob, sparkSqlJob, stepId);
        }
    }
}
