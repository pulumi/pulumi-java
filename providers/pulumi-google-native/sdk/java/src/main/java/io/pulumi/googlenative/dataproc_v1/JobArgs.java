// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package io.pulumi.googlenative.dataproc_v1;

import io.pulumi.core.Output;
import io.pulumi.core.annotations.InputImport;
import io.pulumi.googlenative.dataproc_v1.inputs.HadoopJobArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.HiveJobArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.JobPlacementArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.JobReferenceArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.JobSchedulingArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.PigJobArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.PrestoJobArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.PySparkJobArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.SparkJobArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.SparkRJobArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.SparkSqlJobArgs;
import java.lang.String;
import java.util.Map;
import java.util.Objects;
import javax.annotation.Nullable;


public final class JobArgs extends io.pulumi.resources.ResourceArgs {

    public static final JobArgs Empty = new JobArgs();

    /**
     * Optional. Job is a Hadoop job.
     * 
     */
    @InputImport(name="hadoopJob")
      private final @Nullable Output<HadoopJobArgs> hadoopJob;

    public Output<HadoopJobArgs> getHadoopJob() {
        return this.hadoopJob == null ? Output.empty() : this.hadoopJob;
    }

    /**
     * Optional. Job is a Hive job.
     * 
     */
    @InputImport(name="hiveJob")
      private final @Nullable Output<HiveJobArgs> hiveJob;

    public Output<HiveJobArgs> getHiveJob() {
        return this.hiveJob == null ? Output.empty() : this.hiveJob;
    }

    /**
     * Optional. The labels to associate with this job. Label keys must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values may be empty, but, if present, must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated with a job.
     * 
     */
    @InputImport(name="labels")
      private final @Nullable Output<Map<String,String>> labels;

    public Output<Map<String,String>> getLabels() {
        return this.labels == null ? Output.empty() : this.labels;
    }

    /**
     * Optional. Job is a Pig job.
     * 
     */
    @InputImport(name="pigJob")
      private final @Nullable Output<PigJobArgs> pigJob;

    public Output<PigJobArgs> getPigJob() {
        return this.pigJob == null ? Output.empty() : this.pigJob;
    }

    /**
     * Job information, including how, when, and where to run the job.
     * 
     */
    @InputImport(name="placement", required=true)
      private final Output<JobPlacementArgs> placement;

    public Output<JobPlacementArgs> getPlacement() {
        return this.placement;
    }

    /**
     * Optional. Job is a Presto job.
     * 
     */
    @InputImport(name="prestoJob")
      private final @Nullable Output<PrestoJobArgs> prestoJob;

    public Output<PrestoJobArgs> getPrestoJob() {
        return this.prestoJob == null ? Output.empty() : this.prestoJob;
    }

    @InputImport(name="project")
      private final @Nullable Output<String> project;

    public Output<String> getProject() {
        return this.project == null ? Output.empty() : this.project;
    }

    /**
     * Optional. Job is a PySpark job.
     * 
     */
    @InputImport(name="pysparkJob")
      private final @Nullable Output<PySparkJobArgs> pysparkJob;

    public Output<PySparkJobArgs> getPysparkJob() {
        return this.pysparkJob == null ? Output.empty() : this.pysparkJob;
    }

    /**
     * Optional. The fully qualified reference to the job, which can be used to obtain the equivalent REST path of the job resource. If this property is not specified when a job is created, the server generates a job_id.
     * 
     */
    @InputImport(name="reference")
      private final @Nullable Output<JobReferenceArgs> reference;

    public Output<JobReferenceArgs> getReference() {
        return this.reference == null ? Output.empty() : this.reference;
    }

    @InputImport(name="region", required=true)
      private final Output<String> region;

    public Output<String> getRegion() {
        return this.region;
    }

    /**
     * Optional. A unique id used to identify the request. If the server receives two SubmitJobRequest (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.SubmitJobRequest)s with the same id, then the second request will be ignored and the first Job created and stored in the backend is returned.It is recommended to always set this value to a UUID (https://en.wikipedia.org/wiki/Universally_unique_identifier).The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). The maximum length is 40 characters.
     * 
     */
    @InputImport(name="requestId")
      private final @Nullable Output<String> requestId;

    public Output<String> getRequestId() {
        return this.requestId == null ? Output.empty() : this.requestId;
    }

    /**
     * Optional. Job scheduling configuration.
     * 
     */
    @InputImport(name="scheduling")
      private final @Nullable Output<JobSchedulingArgs> scheduling;

    public Output<JobSchedulingArgs> getScheduling() {
        return this.scheduling == null ? Output.empty() : this.scheduling;
    }

    /**
     * Optional. Job is a Spark job.
     * 
     */
    @InputImport(name="sparkJob")
      private final @Nullable Output<SparkJobArgs> sparkJob;

    public Output<SparkJobArgs> getSparkJob() {
        return this.sparkJob == null ? Output.empty() : this.sparkJob;
    }

    /**
     * Optional. Job is a SparkR job.
     * 
     */
    @InputImport(name="sparkRJob")
      private final @Nullable Output<SparkRJobArgs> sparkRJob;

    public Output<SparkRJobArgs> getSparkRJob() {
        return this.sparkRJob == null ? Output.empty() : this.sparkRJob;
    }

    /**
     * Optional. Job is a SparkSql job.
     * 
     */
    @InputImport(name="sparkSqlJob")
      private final @Nullable Output<SparkSqlJobArgs> sparkSqlJob;

    public Output<SparkSqlJobArgs> getSparkSqlJob() {
        return this.sparkSqlJob == null ? Output.empty() : this.sparkSqlJob;
    }

    public JobArgs(
        @Nullable Output<HadoopJobArgs> hadoopJob,
        @Nullable Output<HiveJobArgs> hiveJob,
        @Nullable Output<Map<String,String>> labels,
        @Nullable Output<PigJobArgs> pigJob,
        Output<JobPlacementArgs> placement,
        @Nullable Output<PrestoJobArgs> prestoJob,
        @Nullable Output<String> project,
        @Nullable Output<PySparkJobArgs> pysparkJob,
        @Nullable Output<JobReferenceArgs> reference,
        Output<String> region,
        @Nullable Output<String> requestId,
        @Nullable Output<JobSchedulingArgs> scheduling,
        @Nullable Output<SparkJobArgs> sparkJob,
        @Nullable Output<SparkRJobArgs> sparkRJob,
        @Nullable Output<SparkSqlJobArgs> sparkSqlJob) {
        this.hadoopJob = hadoopJob;
        this.hiveJob = hiveJob;
        this.labels = labels;
        this.pigJob = pigJob;
        this.placement = Objects.requireNonNull(placement, "expected parameter 'placement' to be non-null");
        this.prestoJob = prestoJob;
        this.project = project;
        this.pysparkJob = pysparkJob;
        this.reference = reference;
        this.region = Objects.requireNonNull(region, "expected parameter 'region' to be non-null");
        this.requestId = requestId;
        this.scheduling = scheduling;
        this.sparkJob = sparkJob;
        this.sparkRJob = sparkRJob;
        this.sparkSqlJob = sparkSqlJob;
    }

    private JobArgs() {
        this.hadoopJob = Output.empty();
        this.hiveJob = Output.empty();
        this.labels = Output.empty();
        this.pigJob = Output.empty();
        this.placement = Output.empty();
        this.prestoJob = Output.empty();
        this.project = Output.empty();
        this.pysparkJob = Output.empty();
        this.reference = Output.empty();
        this.region = Output.empty();
        this.requestId = Output.empty();
        this.scheduling = Output.empty();
        this.sparkJob = Output.empty();
        this.sparkRJob = Output.empty();
        this.sparkSqlJob = Output.empty();
    }

    public static Builder builder() {
        return new Builder();
    }

    public static Builder builder(JobArgs defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private @Nullable Output<HadoopJobArgs> hadoopJob;
        private @Nullable Output<HiveJobArgs> hiveJob;
        private @Nullable Output<Map<String,String>> labels;
        private @Nullable Output<PigJobArgs> pigJob;
        private Output<JobPlacementArgs> placement;
        private @Nullable Output<PrestoJobArgs> prestoJob;
        private @Nullable Output<String> project;
        private @Nullable Output<PySparkJobArgs> pysparkJob;
        private @Nullable Output<JobReferenceArgs> reference;
        private Output<String> region;
        private @Nullable Output<String> requestId;
        private @Nullable Output<JobSchedulingArgs> scheduling;
        private @Nullable Output<SparkJobArgs> sparkJob;
        private @Nullable Output<SparkRJobArgs> sparkRJob;
        private @Nullable Output<SparkSqlJobArgs> sparkSqlJob;

        public Builder() {
    	      // Empty
        }

        public Builder(JobArgs defaults) {
    	      Objects.requireNonNull(defaults);
    	      this.hadoopJob = defaults.hadoopJob;
    	      this.hiveJob = defaults.hiveJob;
    	      this.labels = defaults.labels;
    	      this.pigJob = defaults.pigJob;
    	      this.placement = defaults.placement;
    	      this.prestoJob = defaults.prestoJob;
    	      this.project = defaults.project;
    	      this.pysparkJob = defaults.pysparkJob;
    	      this.reference = defaults.reference;
    	      this.region = defaults.region;
    	      this.requestId = defaults.requestId;
    	      this.scheduling = defaults.scheduling;
    	      this.sparkJob = defaults.sparkJob;
    	      this.sparkRJob = defaults.sparkRJob;
    	      this.sparkSqlJob = defaults.sparkSqlJob;
        }

        public Builder hadoopJob(@Nullable Output<HadoopJobArgs> hadoopJob) {
            this.hadoopJob = hadoopJob;
            return this;
        }

        public Builder hadoopJob(@Nullable HadoopJobArgs hadoopJob) {
            this.hadoopJob = Output.ofNullable(hadoopJob);
            return this;
        }

        public Builder hiveJob(@Nullable Output<HiveJobArgs> hiveJob) {
            this.hiveJob = hiveJob;
            return this;
        }

        public Builder hiveJob(@Nullable HiveJobArgs hiveJob) {
            this.hiveJob = Output.ofNullable(hiveJob);
            return this;
        }

        public Builder labels(@Nullable Output<Map<String,String>> labels) {
            this.labels = labels;
            return this;
        }

        public Builder labels(@Nullable Map<String,String> labels) {
            this.labels = Output.ofNullable(labels);
            return this;
        }

        public Builder pigJob(@Nullable Output<PigJobArgs> pigJob) {
            this.pigJob = pigJob;
            return this;
        }

        public Builder pigJob(@Nullable PigJobArgs pigJob) {
            this.pigJob = Output.ofNullable(pigJob);
            return this;
        }

        public Builder placement(Output<JobPlacementArgs> placement) {
            this.placement = Objects.requireNonNull(placement);
            return this;
        }

        public Builder placement(JobPlacementArgs placement) {
            this.placement = Output.of(Objects.requireNonNull(placement));
            return this;
        }

        public Builder prestoJob(@Nullable Output<PrestoJobArgs> prestoJob) {
            this.prestoJob = prestoJob;
            return this;
        }

        public Builder prestoJob(@Nullable PrestoJobArgs prestoJob) {
            this.prestoJob = Output.ofNullable(prestoJob);
            return this;
        }

        public Builder project(@Nullable Output<String> project) {
            this.project = project;
            return this;
        }

        public Builder project(@Nullable String project) {
            this.project = Output.ofNullable(project);
            return this;
        }

        public Builder pysparkJob(@Nullable Output<PySparkJobArgs> pysparkJob) {
            this.pysparkJob = pysparkJob;
            return this;
        }

        public Builder pysparkJob(@Nullable PySparkJobArgs pysparkJob) {
            this.pysparkJob = Output.ofNullable(pysparkJob);
            return this;
        }

        public Builder reference(@Nullable Output<JobReferenceArgs> reference) {
            this.reference = reference;
            return this;
        }

        public Builder reference(@Nullable JobReferenceArgs reference) {
            this.reference = Output.ofNullable(reference);
            return this;
        }

        public Builder region(Output<String> region) {
            this.region = Objects.requireNonNull(region);
            return this;
        }

        public Builder region(String region) {
            this.region = Output.of(Objects.requireNonNull(region));
            return this;
        }

        public Builder requestId(@Nullable Output<String> requestId) {
            this.requestId = requestId;
            return this;
        }

        public Builder requestId(@Nullable String requestId) {
            this.requestId = Output.ofNullable(requestId);
            return this;
        }

        public Builder scheduling(@Nullable Output<JobSchedulingArgs> scheduling) {
            this.scheduling = scheduling;
            return this;
        }

        public Builder scheduling(@Nullable JobSchedulingArgs scheduling) {
            this.scheduling = Output.ofNullable(scheduling);
            return this;
        }

        public Builder sparkJob(@Nullable Output<SparkJobArgs> sparkJob) {
            this.sparkJob = sparkJob;
            return this;
        }

        public Builder sparkJob(@Nullable SparkJobArgs sparkJob) {
            this.sparkJob = Output.ofNullable(sparkJob);
            return this;
        }

        public Builder sparkRJob(@Nullable Output<SparkRJobArgs> sparkRJob) {
            this.sparkRJob = sparkRJob;
            return this;
        }

        public Builder sparkRJob(@Nullable SparkRJobArgs sparkRJob) {
            this.sparkRJob = Output.ofNullable(sparkRJob);
            return this;
        }

        public Builder sparkSqlJob(@Nullable Output<SparkSqlJobArgs> sparkSqlJob) {
            this.sparkSqlJob = sparkSqlJob;
            return this;
        }

        public Builder sparkSqlJob(@Nullable SparkSqlJobArgs sparkSqlJob) {
            this.sparkSqlJob = Output.ofNullable(sparkSqlJob);
            return this;
        }
        public JobArgs build() {
            return new JobArgs(hadoopJob, hiveJob, labels, pigJob, placement, prestoJob, project, pysparkJob, reference, region, requestId, scheduling, sparkJob, sparkRJob, sparkSqlJob);
        }
    }
}
