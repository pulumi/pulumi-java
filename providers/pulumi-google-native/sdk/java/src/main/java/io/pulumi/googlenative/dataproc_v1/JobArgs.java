// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package io.pulumi.googlenative.dataproc_v1;

import io.pulumi.core.Input;
import io.pulumi.core.annotations.InputImport;
import io.pulumi.googlenative.dataproc_v1.inputs.HadoopJobArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.HiveJobArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.JobPlacementArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.JobReferenceArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.JobSchedulingArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.PigJobArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.PrestoJobArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.PySparkJobArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.SparkJobArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.SparkRJobArgs;
import io.pulumi.googlenative.dataproc_v1.inputs.SparkSqlJobArgs;
import java.lang.String;
import java.util.Map;
import java.util.Objects;
import javax.annotation.Nullable;


public final class JobArgs extends io.pulumi.resources.ResourceArgs {

    public static final JobArgs Empty = new JobArgs();

    /**
     * Optional. Job is a Hadoop job.
     * 
     */
    @InputImport(name="hadoopJob")
    private final @Nullable Input<HadoopJobArgs> hadoopJob;

    public Input<HadoopJobArgs> getHadoopJob() {
        return this.hadoopJob == null ? Input.empty() : this.hadoopJob;
    }

    /**
     * Optional. Job is a Hive job.
     * 
     */
    @InputImport(name="hiveJob")
    private final @Nullable Input<HiveJobArgs> hiveJob;

    public Input<HiveJobArgs> getHiveJob() {
        return this.hiveJob == null ? Input.empty() : this.hiveJob;
    }

    /**
     * Optional. The labels to associate with this job. Label keys must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). Label values may be empty, but, if present, must contain 1 to 63 characters, and must conform to RFC 1035 (https://www.ietf.org/rfc/rfc1035.txt). No more than 32 labels can be associated with a job.
     * 
     */
    @InputImport(name="labels")
    private final @Nullable Input<Map<String,String>> labels;

    public Input<Map<String,String>> getLabels() {
        return this.labels == null ? Input.empty() : this.labels;
    }

    /**
     * Optional. Job is a Pig job.
     * 
     */
    @InputImport(name="pigJob")
    private final @Nullable Input<PigJobArgs> pigJob;

    public Input<PigJobArgs> getPigJob() {
        return this.pigJob == null ? Input.empty() : this.pigJob;
    }

    /**
     * Job information, including how, when, and where to run the job.
     * 
     */
    @InputImport(name="placement", required=true)
    private final Input<JobPlacementArgs> placement;

    public Input<JobPlacementArgs> getPlacement() {
        return this.placement;
    }

    /**
     * Optional. Job is a Presto job.
     * 
     */
    @InputImport(name="prestoJob")
    private final @Nullable Input<PrestoJobArgs> prestoJob;

    public Input<PrestoJobArgs> getPrestoJob() {
        return this.prestoJob == null ? Input.empty() : this.prestoJob;
    }

    @InputImport(name="project")
    private final @Nullable Input<String> project;

    public Input<String> getProject() {
        return this.project == null ? Input.empty() : this.project;
    }

    /**
     * Optional. Job is a PySpark job.
     * 
     */
    @InputImport(name="pysparkJob")
    private final @Nullable Input<PySparkJobArgs> pysparkJob;

    public Input<PySparkJobArgs> getPysparkJob() {
        return this.pysparkJob == null ? Input.empty() : this.pysparkJob;
    }

    /**
     * Optional. The fully qualified reference to the job, which can be used to obtain the equivalent REST path of the job resource. If this property is not specified when a job is created, the server generates a job_id.
     * 
     */
    @InputImport(name="reference")
    private final @Nullable Input<JobReferenceArgs> reference;

    public Input<JobReferenceArgs> getReference() {
        return this.reference == null ? Input.empty() : this.reference;
    }

    @InputImport(name="region", required=true)
    private final Input<String> region;

    public Input<String> getRegion() {
        return this.region;
    }

    /**
     * Optional. A unique id used to identify the request. If the server receives two SubmitJobRequest (https://cloud.google.com/dataproc/docs/reference/rpc/google.cloud.dataproc.v1#google.cloud.dataproc.v1.SubmitJobRequest)s with the same id, then the second request will be ignored and the first Job created and stored in the backend is returned.It is recommended to always set this value to a UUID (https://en.wikipedia.org/wiki/Universally_unique_identifier).The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). The maximum length is 40 characters.
     * 
     */
    @InputImport(name="requestId")
    private final @Nullable Input<String> requestId;

    public Input<String> getRequestId() {
        return this.requestId == null ? Input.empty() : this.requestId;
    }

    /**
     * Optional. Job scheduling configuration.
     * 
     */
    @InputImport(name="scheduling")
    private final @Nullable Input<JobSchedulingArgs> scheduling;

    public Input<JobSchedulingArgs> getScheduling() {
        return this.scheduling == null ? Input.empty() : this.scheduling;
    }

    /**
     * Optional. Job is a Spark job.
     * 
     */
    @InputImport(name="sparkJob")
    private final @Nullable Input<SparkJobArgs> sparkJob;

    public Input<SparkJobArgs> getSparkJob() {
        return this.sparkJob == null ? Input.empty() : this.sparkJob;
    }

    /**
     * Optional. Job is a SparkR job.
     * 
     */
    @InputImport(name="sparkRJob")
    private final @Nullable Input<SparkRJobArgs> sparkRJob;

    public Input<SparkRJobArgs> getSparkRJob() {
        return this.sparkRJob == null ? Input.empty() : this.sparkRJob;
    }

    /**
     * Optional. Job is a SparkSql job.
     * 
     */
    @InputImport(name="sparkSqlJob")
    private final @Nullable Input<SparkSqlJobArgs> sparkSqlJob;

    public Input<SparkSqlJobArgs> getSparkSqlJob() {
        return this.sparkSqlJob == null ? Input.empty() : this.sparkSqlJob;
    }

    public JobArgs(
        @Nullable Input<HadoopJobArgs> hadoopJob,
        @Nullable Input<HiveJobArgs> hiveJob,
        @Nullable Input<Map<String,String>> labels,
        @Nullable Input<PigJobArgs> pigJob,
        Input<JobPlacementArgs> placement,
        @Nullable Input<PrestoJobArgs> prestoJob,
        @Nullable Input<String> project,
        @Nullable Input<PySparkJobArgs> pysparkJob,
        @Nullable Input<JobReferenceArgs> reference,
        Input<String> region,
        @Nullable Input<String> requestId,
        @Nullable Input<JobSchedulingArgs> scheduling,
        @Nullable Input<SparkJobArgs> sparkJob,
        @Nullable Input<SparkRJobArgs> sparkRJob,
        @Nullable Input<SparkSqlJobArgs> sparkSqlJob) {
        this.hadoopJob = hadoopJob;
        this.hiveJob = hiveJob;
        this.labels = labels;
        this.pigJob = pigJob;
        this.placement = Objects.requireNonNull(placement, "expected parameter 'placement' to be non-null");
        this.prestoJob = prestoJob;
        this.project = project;
        this.pysparkJob = pysparkJob;
        this.reference = reference;
        this.region = Objects.requireNonNull(region, "expected parameter 'region' to be non-null");
        this.requestId = requestId;
        this.scheduling = scheduling;
        this.sparkJob = sparkJob;
        this.sparkRJob = sparkRJob;
        this.sparkSqlJob = sparkSqlJob;
    }

    private JobArgs() {
        this.hadoopJob = Input.empty();
        this.hiveJob = Input.empty();
        this.labels = Input.empty();
        this.pigJob = Input.empty();
        this.placement = Input.empty();
        this.prestoJob = Input.empty();
        this.project = Input.empty();
        this.pysparkJob = Input.empty();
        this.reference = Input.empty();
        this.region = Input.empty();
        this.requestId = Input.empty();
        this.scheduling = Input.empty();
        this.sparkJob = Input.empty();
        this.sparkRJob = Input.empty();
        this.sparkSqlJob = Input.empty();
    }

    public static Builder builder() {
        return new Builder();
    }

    public static Builder builder(JobArgs defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private @Nullable Input<HadoopJobArgs> hadoopJob;
        private @Nullable Input<HiveJobArgs> hiveJob;
        private @Nullable Input<Map<String,String>> labels;
        private @Nullable Input<PigJobArgs> pigJob;
        private Input<JobPlacementArgs> placement;
        private @Nullable Input<PrestoJobArgs> prestoJob;
        private @Nullable Input<String> project;
        private @Nullable Input<PySparkJobArgs> pysparkJob;
        private @Nullable Input<JobReferenceArgs> reference;
        private Input<String> region;
        private @Nullable Input<String> requestId;
        private @Nullable Input<JobSchedulingArgs> scheduling;
        private @Nullable Input<SparkJobArgs> sparkJob;
        private @Nullable Input<SparkRJobArgs> sparkRJob;
        private @Nullable Input<SparkSqlJobArgs> sparkSqlJob;

        public Builder() {
    	      // Empty
        }

        public Builder(JobArgs defaults) {
    	      Objects.requireNonNull(defaults);
    	      this.hadoopJob = defaults.hadoopJob;
    	      this.hiveJob = defaults.hiveJob;
    	      this.labels = defaults.labels;
    	      this.pigJob = defaults.pigJob;
    	      this.placement = defaults.placement;
    	      this.prestoJob = defaults.prestoJob;
    	      this.project = defaults.project;
    	      this.pysparkJob = defaults.pysparkJob;
    	      this.reference = defaults.reference;
    	      this.region = defaults.region;
    	      this.requestId = defaults.requestId;
    	      this.scheduling = defaults.scheduling;
    	      this.sparkJob = defaults.sparkJob;
    	      this.sparkRJob = defaults.sparkRJob;
    	      this.sparkSqlJob = defaults.sparkSqlJob;
        }

        public Builder setHadoopJob(@Nullable Input<HadoopJobArgs> hadoopJob) {
            this.hadoopJob = hadoopJob;
            return this;
        }

        public Builder setHadoopJob(@Nullable HadoopJobArgs hadoopJob) {
            this.hadoopJob = Input.ofNullable(hadoopJob);
            return this;
        }

        public Builder setHiveJob(@Nullable Input<HiveJobArgs> hiveJob) {
            this.hiveJob = hiveJob;
            return this;
        }

        public Builder setHiveJob(@Nullable HiveJobArgs hiveJob) {
            this.hiveJob = Input.ofNullable(hiveJob);
            return this;
        }

        public Builder setLabels(@Nullable Input<Map<String,String>> labels) {
            this.labels = labels;
            return this;
        }

        public Builder setLabels(@Nullable Map<String,String> labels) {
            this.labels = Input.ofNullable(labels);
            return this;
        }

        public Builder setPigJob(@Nullable Input<PigJobArgs> pigJob) {
            this.pigJob = pigJob;
            return this;
        }

        public Builder setPigJob(@Nullable PigJobArgs pigJob) {
            this.pigJob = Input.ofNullable(pigJob);
            return this;
        }

        public Builder setPlacement(Input<JobPlacementArgs> placement) {
            this.placement = Objects.requireNonNull(placement);
            return this;
        }

        public Builder setPlacement(JobPlacementArgs placement) {
            this.placement = Input.of(Objects.requireNonNull(placement));
            return this;
        }

        public Builder setPrestoJob(@Nullable Input<PrestoJobArgs> prestoJob) {
            this.prestoJob = prestoJob;
            return this;
        }

        public Builder setPrestoJob(@Nullable PrestoJobArgs prestoJob) {
            this.prestoJob = Input.ofNullable(prestoJob);
            return this;
        }

        public Builder setProject(@Nullable Input<String> project) {
            this.project = project;
            return this;
        }

        public Builder setProject(@Nullable String project) {
            this.project = Input.ofNullable(project);
            return this;
        }

        public Builder setPysparkJob(@Nullable Input<PySparkJobArgs> pysparkJob) {
            this.pysparkJob = pysparkJob;
            return this;
        }

        public Builder setPysparkJob(@Nullable PySparkJobArgs pysparkJob) {
            this.pysparkJob = Input.ofNullable(pysparkJob);
            return this;
        }

        public Builder setReference(@Nullable Input<JobReferenceArgs> reference) {
            this.reference = reference;
            return this;
        }

        public Builder setReference(@Nullable JobReferenceArgs reference) {
            this.reference = Input.ofNullable(reference);
            return this;
        }

        public Builder setRegion(Input<String> region) {
            this.region = Objects.requireNonNull(region);
            return this;
        }

        public Builder setRegion(String region) {
            this.region = Input.of(Objects.requireNonNull(region));
            return this;
        }

        public Builder setRequestId(@Nullable Input<String> requestId) {
            this.requestId = requestId;
            return this;
        }

        public Builder setRequestId(@Nullable String requestId) {
            this.requestId = Input.ofNullable(requestId);
            return this;
        }

        public Builder setScheduling(@Nullable Input<JobSchedulingArgs> scheduling) {
            this.scheduling = scheduling;
            return this;
        }

        public Builder setScheduling(@Nullable JobSchedulingArgs scheduling) {
            this.scheduling = Input.ofNullable(scheduling);
            return this;
        }

        public Builder setSparkJob(@Nullable Input<SparkJobArgs> sparkJob) {
            this.sparkJob = sparkJob;
            return this;
        }

        public Builder setSparkJob(@Nullable SparkJobArgs sparkJob) {
            this.sparkJob = Input.ofNullable(sparkJob);
            return this;
        }

        public Builder setSparkRJob(@Nullable Input<SparkRJobArgs> sparkRJob) {
            this.sparkRJob = sparkRJob;
            return this;
        }

        public Builder setSparkRJob(@Nullable SparkRJobArgs sparkRJob) {
            this.sparkRJob = Input.ofNullable(sparkRJob);
            return this;
        }

        public Builder setSparkSqlJob(@Nullable Input<SparkSqlJobArgs> sparkSqlJob) {
            this.sparkSqlJob = sparkSqlJob;
            return this;
        }

        public Builder setSparkSqlJob(@Nullable SparkSqlJobArgs sparkSqlJob) {
            this.sparkSqlJob = Input.ofNullable(sparkSqlJob);
            return this;
        }

        public JobArgs build() {
            return new JobArgs(hadoopJob, hiveJob, labels, pigJob, placement, prestoJob, project, pysparkJob, reference, region, requestId, scheduling, sparkJob, sparkRJob, sparkSqlJob);
        }
    }
}
