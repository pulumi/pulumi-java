// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package io.pulumi.gcp.dataproc;

import io.pulumi.core.Output;
import io.pulumi.core.annotations.Import;
import io.pulumi.core.internal.Codegen;
import io.pulumi.gcp.dataproc.inputs.JobHadoopConfigArgs;
import io.pulumi.gcp.dataproc.inputs.JobHiveConfigArgs;
import io.pulumi.gcp.dataproc.inputs.JobPigConfigArgs;
import io.pulumi.gcp.dataproc.inputs.JobPlacementArgs;
import io.pulumi.gcp.dataproc.inputs.JobPysparkConfigArgs;
import io.pulumi.gcp.dataproc.inputs.JobReferenceArgs;
import io.pulumi.gcp.dataproc.inputs.JobSchedulingArgs;
import io.pulumi.gcp.dataproc.inputs.JobSparkConfigArgs;
import io.pulumi.gcp.dataproc.inputs.JobSparksqlConfigArgs;
import java.lang.Boolean;
import java.lang.String;
import java.util.Map;
import java.util.Objects;
import javax.annotation.Nullable;


public final class JobArgs extends io.pulumi.resources.ResourceArgs {

    public static final JobArgs Empty = new JobArgs();

    /**
     * By default, you can only delete inactive jobs within
     * Dataproc. Setting this to true, and calling destroy, will ensure that the
     * job is first cancelled before issuing the delete.
     * 
     */
    @Import(name="forceDelete")
      private final @Nullable Output<Boolean> forceDelete;

    public Output<Boolean> getForceDelete() {
        return this.forceDelete == null ? Codegen.empty() : this.forceDelete;
    }

    /**
     * The config of Hadoop job
     * 
     */
    @Import(name="hadoopConfig")
      private final @Nullable Output<JobHadoopConfigArgs> hadoopConfig;

    public Output<JobHadoopConfigArgs> getHadoopConfig() {
        return this.hadoopConfig == null ? Codegen.empty() : this.hadoopConfig;
    }

    /**
     * The config of hive job
     * 
     */
    @Import(name="hiveConfig")
      private final @Nullable Output<JobHiveConfigArgs> hiveConfig;

    public Output<JobHiveConfigArgs> getHiveConfig() {
        return this.hiveConfig == null ? Codegen.empty() : this.hiveConfig;
    }

    /**
     * The list of labels (key/value pairs) to add to the job.
     * 
     */
    @Import(name="labels")
      private final @Nullable Output<Map<String,String>> labels;

    public Output<Map<String,String>> getLabels() {
        return this.labels == null ? Codegen.empty() : this.labels;
    }

    /**
     * The config of pag job.
     * 
     */
    @Import(name="pigConfig")
      private final @Nullable Output<JobPigConfigArgs> pigConfig;

    public Output<JobPigConfigArgs> getPigConfig() {
        return this.pigConfig == null ? Codegen.empty() : this.pigConfig;
    }

    /**
     * The config of job placement.
     * 
     */
    @Import(name="placement", required=true)
      private final Output<JobPlacementArgs> placement;

    public Output<JobPlacementArgs> getPlacement() {
        return this.placement;
    }

    /**
     * The project in which the `cluster` can be found and jobs
     * subsequently run against. If it is not provided, the provider project is used.
     * 
     */
    @Import(name="project")
      private final @Nullable Output<String> project;

    public Output<String> getProject() {
        return this.project == null ? Codegen.empty() : this.project;
    }

    /**
     * The config of pySpark job.
     * 
     */
    @Import(name="pysparkConfig")
      private final @Nullable Output<JobPysparkConfigArgs> pysparkConfig;

    public Output<JobPysparkConfigArgs> getPysparkConfig() {
        return this.pysparkConfig == null ? Codegen.empty() : this.pysparkConfig;
    }

    /**
     * The reference of the job
     * 
     */
    @Import(name="reference")
      private final @Nullable Output<JobReferenceArgs> reference;

    public Output<JobReferenceArgs> getReference() {
        return this.reference == null ? Codegen.empty() : this.reference;
    }

    /**
     * The Cloud Dataproc region. This essentially determines which clusters are available
     * for this job to be submitted to. If not specified, defaults to `global`.
     * 
     */
    @Import(name="region")
      private final @Nullable Output<String> region;

    public Output<String> getRegion() {
        return this.region == null ? Codegen.empty() : this.region;
    }

    /**
     * Optional. Job scheduling configuration.
     * 
     */
    @Import(name="scheduling")
      private final @Nullable Output<JobSchedulingArgs> scheduling;

    public Output<JobSchedulingArgs> getScheduling() {
        return this.scheduling == null ? Codegen.empty() : this.scheduling;
    }

    /**
     * The config of the Spark job.
     * 
     */
    @Import(name="sparkConfig")
      private final @Nullable Output<JobSparkConfigArgs> sparkConfig;

    public Output<JobSparkConfigArgs> getSparkConfig() {
        return this.sparkConfig == null ? Codegen.empty() : this.sparkConfig;
    }

    /**
     * The config of SparkSql job
     * 
     */
    @Import(name="sparksqlConfig")
      private final @Nullable Output<JobSparksqlConfigArgs> sparksqlConfig;

    public Output<JobSparksqlConfigArgs> getSparksqlConfig() {
        return this.sparksqlConfig == null ? Codegen.empty() : this.sparksqlConfig;
    }

    public JobArgs(
        @Nullable Output<Boolean> forceDelete,
        @Nullable Output<JobHadoopConfigArgs> hadoopConfig,
        @Nullable Output<JobHiveConfigArgs> hiveConfig,
        @Nullable Output<Map<String,String>> labels,
        @Nullable Output<JobPigConfigArgs> pigConfig,
        Output<JobPlacementArgs> placement,
        @Nullable Output<String> project,
        @Nullable Output<JobPysparkConfigArgs> pysparkConfig,
        @Nullable Output<JobReferenceArgs> reference,
        @Nullable Output<String> region,
        @Nullable Output<JobSchedulingArgs> scheduling,
        @Nullable Output<JobSparkConfigArgs> sparkConfig,
        @Nullable Output<JobSparksqlConfigArgs> sparksqlConfig) {
        this.forceDelete = forceDelete;
        this.hadoopConfig = hadoopConfig;
        this.hiveConfig = hiveConfig;
        this.labels = labels;
        this.pigConfig = pigConfig;
        this.placement = Objects.requireNonNull(placement, "expected parameter 'placement' to be non-null");
        this.project = project;
        this.pysparkConfig = pysparkConfig;
        this.reference = reference;
        this.region = region;
        this.scheduling = scheduling;
        this.sparkConfig = sparkConfig;
        this.sparksqlConfig = sparksqlConfig;
    }

    private JobArgs() {
        this.forceDelete = Codegen.empty();
        this.hadoopConfig = Codegen.empty();
        this.hiveConfig = Codegen.empty();
        this.labels = Codegen.empty();
        this.pigConfig = Codegen.empty();
        this.placement = Codegen.empty();
        this.project = Codegen.empty();
        this.pysparkConfig = Codegen.empty();
        this.reference = Codegen.empty();
        this.region = Codegen.empty();
        this.scheduling = Codegen.empty();
        this.sparkConfig = Codegen.empty();
        this.sparksqlConfig = Codegen.empty();
    }

    public static Builder builder() {
        return new Builder();
    }

    public static Builder builder(JobArgs defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private @Nullable Output<Boolean> forceDelete;
        private @Nullable Output<JobHadoopConfigArgs> hadoopConfig;
        private @Nullable Output<JobHiveConfigArgs> hiveConfig;
        private @Nullable Output<Map<String,String>> labels;
        private @Nullable Output<JobPigConfigArgs> pigConfig;
        private Output<JobPlacementArgs> placement;
        private @Nullable Output<String> project;
        private @Nullable Output<JobPysparkConfigArgs> pysparkConfig;
        private @Nullable Output<JobReferenceArgs> reference;
        private @Nullable Output<String> region;
        private @Nullable Output<JobSchedulingArgs> scheduling;
        private @Nullable Output<JobSparkConfigArgs> sparkConfig;
        private @Nullable Output<JobSparksqlConfigArgs> sparksqlConfig;

        public Builder() {
    	      // Empty
        }

        public Builder(JobArgs defaults) {
    	      Objects.requireNonNull(defaults);
    	      this.forceDelete = defaults.forceDelete;
    	      this.hadoopConfig = defaults.hadoopConfig;
    	      this.hiveConfig = defaults.hiveConfig;
    	      this.labels = defaults.labels;
    	      this.pigConfig = defaults.pigConfig;
    	      this.placement = defaults.placement;
    	      this.project = defaults.project;
    	      this.pysparkConfig = defaults.pysparkConfig;
    	      this.reference = defaults.reference;
    	      this.region = defaults.region;
    	      this.scheduling = defaults.scheduling;
    	      this.sparkConfig = defaults.sparkConfig;
    	      this.sparksqlConfig = defaults.sparksqlConfig;
        }

        public Builder forceDelete(@Nullable Output<Boolean> forceDelete) {
            this.forceDelete = forceDelete;
            return this;
        }
        public Builder forceDelete(@Nullable Boolean forceDelete) {
            this.forceDelete = Codegen.ofNullable(forceDelete);
            return this;
        }
        public Builder hadoopConfig(@Nullable Output<JobHadoopConfigArgs> hadoopConfig) {
            this.hadoopConfig = hadoopConfig;
            return this;
        }
        public Builder hadoopConfig(@Nullable JobHadoopConfigArgs hadoopConfig) {
            this.hadoopConfig = Codegen.ofNullable(hadoopConfig);
            return this;
        }
        public Builder hiveConfig(@Nullable Output<JobHiveConfigArgs> hiveConfig) {
            this.hiveConfig = hiveConfig;
            return this;
        }
        public Builder hiveConfig(@Nullable JobHiveConfigArgs hiveConfig) {
            this.hiveConfig = Codegen.ofNullable(hiveConfig);
            return this;
        }
        public Builder labels(@Nullable Output<Map<String,String>> labels) {
            this.labels = labels;
            return this;
        }
        public Builder labels(@Nullable Map<String,String> labels) {
            this.labels = Codegen.ofNullable(labels);
            return this;
        }
        public Builder pigConfig(@Nullable Output<JobPigConfigArgs> pigConfig) {
            this.pigConfig = pigConfig;
            return this;
        }
        public Builder pigConfig(@Nullable JobPigConfigArgs pigConfig) {
            this.pigConfig = Codegen.ofNullable(pigConfig);
            return this;
        }
        public Builder placement(Output<JobPlacementArgs> placement) {
            this.placement = Objects.requireNonNull(placement);
            return this;
        }
        public Builder placement(JobPlacementArgs placement) {
            this.placement = Output.of(Objects.requireNonNull(placement));
            return this;
        }
        public Builder project(@Nullable Output<String> project) {
            this.project = project;
            return this;
        }
        public Builder project(@Nullable String project) {
            this.project = Codegen.ofNullable(project);
            return this;
        }
        public Builder pysparkConfig(@Nullable Output<JobPysparkConfigArgs> pysparkConfig) {
            this.pysparkConfig = pysparkConfig;
            return this;
        }
        public Builder pysparkConfig(@Nullable JobPysparkConfigArgs pysparkConfig) {
            this.pysparkConfig = Codegen.ofNullable(pysparkConfig);
            return this;
        }
        public Builder reference(@Nullable Output<JobReferenceArgs> reference) {
            this.reference = reference;
            return this;
        }
        public Builder reference(@Nullable JobReferenceArgs reference) {
            this.reference = Codegen.ofNullable(reference);
            return this;
        }
        public Builder region(@Nullable Output<String> region) {
            this.region = region;
            return this;
        }
        public Builder region(@Nullable String region) {
            this.region = Codegen.ofNullable(region);
            return this;
        }
        public Builder scheduling(@Nullable Output<JobSchedulingArgs> scheduling) {
            this.scheduling = scheduling;
            return this;
        }
        public Builder scheduling(@Nullable JobSchedulingArgs scheduling) {
            this.scheduling = Codegen.ofNullable(scheduling);
            return this;
        }
        public Builder sparkConfig(@Nullable Output<JobSparkConfigArgs> sparkConfig) {
            this.sparkConfig = sparkConfig;
            return this;
        }
        public Builder sparkConfig(@Nullable JobSparkConfigArgs sparkConfig) {
            this.sparkConfig = Codegen.ofNullable(sparkConfig);
            return this;
        }
        public Builder sparksqlConfig(@Nullable Output<JobSparksqlConfigArgs> sparksqlConfig) {
            this.sparksqlConfig = sparksqlConfig;
            return this;
        }
        public Builder sparksqlConfig(@Nullable JobSparksqlConfigArgs sparksqlConfig) {
            this.sparksqlConfig = Codegen.ofNullable(sparksqlConfig);
            return this;
        }        public JobArgs build() {
            return new JobArgs(forceDelete, hadoopConfig, hiveConfig, labels, pigConfig, placement, project, pysparkConfig, reference, region, scheduling, sparkConfig, sparksqlConfig);
        }
    }
}
