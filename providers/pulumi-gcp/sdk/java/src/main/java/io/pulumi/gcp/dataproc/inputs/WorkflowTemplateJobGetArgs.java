// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package io.pulumi.gcp.dataproc.inputs;

import io.pulumi.core.Input;
import io.pulumi.core.annotations.InputImport;
import io.pulumi.gcp.dataproc.inputs.WorkflowTemplateJobHadoopJobGetArgs;
import io.pulumi.gcp.dataproc.inputs.WorkflowTemplateJobHiveJobGetArgs;
import io.pulumi.gcp.dataproc.inputs.WorkflowTemplateJobPigJobGetArgs;
import io.pulumi.gcp.dataproc.inputs.WorkflowTemplateJobPrestoJobGetArgs;
import io.pulumi.gcp.dataproc.inputs.WorkflowTemplateJobPysparkJobGetArgs;
import io.pulumi.gcp.dataproc.inputs.WorkflowTemplateJobSchedulingGetArgs;
import io.pulumi.gcp.dataproc.inputs.WorkflowTemplateJobSparkJobGetArgs;
import io.pulumi.gcp.dataproc.inputs.WorkflowTemplateJobSparkRJobGetArgs;
import io.pulumi.gcp.dataproc.inputs.WorkflowTemplateJobSparkSqlJobGetArgs;
import java.lang.String;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import javax.annotation.Nullable;


public final class WorkflowTemplateJobGetArgs extends io.pulumi.resources.ResourceArgs {

    public static final WorkflowTemplateJobGetArgs Empty = new WorkflowTemplateJobGetArgs();

    /**
     * Optional. Job is a Hadoop job.
     * 
     */
    @InputImport(name="hadoopJob")
        private final @Nullable Input<WorkflowTemplateJobHadoopJobGetArgs> hadoopJob;

    public Input<WorkflowTemplateJobHadoopJobGetArgs> getHadoopJob() {
        return this.hadoopJob == null ? Input.empty() : this.hadoopJob;
    }

    /**
     * Optional. Job is a Hive job.
     * 
     */
    @InputImport(name="hiveJob")
        private final @Nullable Input<WorkflowTemplateJobHiveJobGetArgs> hiveJob;

    public Input<WorkflowTemplateJobHiveJobGetArgs> getHiveJob() {
        return this.hiveJob == null ? Input.empty() : this.hiveJob;
    }

    /**
     * Optional. The labels to associate with this cluster. Label keys must be between 1 and 63 characters long, and must conform to the following PCRE regular expression: {0,63} No more than 32 labels can be associated with a given cluster.
     * 
     */
    @InputImport(name="labels")
        private final @Nullable Input<Map<String,String>> labels;

    public Input<Map<String,String>> getLabels() {
        return this.labels == null ? Input.empty() : this.labels;
    }

    /**
     * Optional. Job is a Pig job.
     * 
     */
    @InputImport(name="pigJob")
        private final @Nullable Input<WorkflowTemplateJobPigJobGetArgs> pigJob;

    public Input<WorkflowTemplateJobPigJobGetArgs> getPigJob() {
        return this.pigJob == null ? Input.empty() : this.pigJob;
    }

    /**
     * Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow.
     * 
     */
    @InputImport(name="prerequisiteStepIds")
        private final @Nullable Input<List<String>> prerequisiteStepIds;

    public Input<List<String>> getPrerequisiteStepIds() {
        return this.prerequisiteStepIds == null ? Input.empty() : this.prerequisiteStepIds;
    }

    /**
     * Optional. Job is a Presto job.
     * 
     */
    @InputImport(name="prestoJob")
        private final @Nullable Input<WorkflowTemplateJobPrestoJobGetArgs> prestoJob;

    public Input<WorkflowTemplateJobPrestoJobGetArgs> getPrestoJob() {
        return this.prestoJob == null ? Input.empty() : this.prestoJob;
    }

    /**
     * Optional. Job is a PySpark job.
     * 
     */
    @InputImport(name="pysparkJob")
        private final @Nullable Input<WorkflowTemplateJobPysparkJobGetArgs> pysparkJob;

    public Input<WorkflowTemplateJobPysparkJobGetArgs> getPysparkJob() {
        return this.pysparkJob == null ? Input.empty() : this.pysparkJob;
    }

    /**
     * Optional. Job scheduling configuration.
     * 
     */
    @InputImport(name="scheduling")
        private final @Nullable Input<WorkflowTemplateJobSchedulingGetArgs> scheduling;

    public Input<WorkflowTemplateJobSchedulingGetArgs> getScheduling() {
        return this.scheduling == null ? Input.empty() : this.scheduling;
    }

    /**
     * Optional. Job is a Spark job.
     * 
     */
    @InputImport(name="sparkJob")
        private final @Nullable Input<WorkflowTemplateJobSparkJobGetArgs> sparkJob;

    public Input<WorkflowTemplateJobSparkJobGetArgs> getSparkJob() {
        return this.sparkJob == null ? Input.empty() : this.sparkJob;
    }

    /**
     * Optional. Job is a SparkR job.
     * 
     */
    @InputImport(name="sparkRJob")
        private final @Nullable Input<WorkflowTemplateJobSparkRJobGetArgs> sparkRJob;

    public Input<WorkflowTemplateJobSparkRJobGetArgs> getSparkRJob() {
        return this.sparkRJob == null ? Input.empty() : this.sparkRJob;
    }

    /**
     * Optional. Job is a SparkSql job.
     * 
     */
    @InputImport(name="sparkSqlJob")
        private final @Nullable Input<WorkflowTemplateJobSparkSqlJobGetArgs> sparkSqlJob;

    public Input<WorkflowTemplateJobSparkSqlJobGetArgs> getSparkSqlJob() {
        return this.sparkSqlJob == null ? Input.empty() : this.sparkSqlJob;
    }

    /**
     * Required. The step id. The id must be unique among all jobs within the template. The step id is used as prefix for job id, as job `goog-dataproc-workflow-step-id` label, and in field from other steps. The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters.
     * 
     */
    @InputImport(name="stepId", required=true)
        private final Input<String> stepId;

    public Input<String> getStepId() {
        return this.stepId;
    }

    public WorkflowTemplateJobGetArgs(
        @Nullable Input<WorkflowTemplateJobHadoopJobGetArgs> hadoopJob,
        @Nullable Input<WorkflowTemplateJobHiveJobGetArgs> hiveJob,
        @Nullable Input<Map<String,String>> labels,
        @Nullable Input<WorkflowTemplateJobPigJobGetArgs> pigJob,
        @Nullable Input<List<String>> prerequisiteStepIds,
        @Nullable Input<WorkflowTemplateJobPrestoJobGetArgs> prestoJob,
        @Nullable Input<WorkflowTemplateJobPysparkJobGetArgs> pysparkJob,
        @Nullable Input<WorkflowTemplateJobSchedulingGetArgs> scheduling,
        @Nullable Input<WorkflowTemplateJobSparkJobGetArgs> sparkJob,
        @Nullable Input<WorkflowTemplateJobSparkRJobGetArgs> sparkRJob,
        @Nullable Input<WorkflowTemplateJobSparkSqlJobGetArgs> sparkSqlJob,
        Input<String> stepId) {
        this.hadoopJob = hadoopJob;
        this.hiveJob = hiveJob;
        this.labels = labels;
        this.pigJob = pigJob;
        this.prerequisiteStepIds = prerequisiteStepIds;
        this.prestoJob = prestoJob;
        this.pysparkJob = pysparkJob;
        this.scheduling = scheduling;
        this.sparkJob = sparkJob;
        this.sparkRJob = sparkRJob;
        this.sparkSqlJob = sparkSqlJob;
        this.stepId = Objects.requireNonNull(stepId, "expected parameter 'stepId' to be non-null");
    }

    private WorkflowTemplateJobGetArgs() {
        this.hadoopJob = Input.empty();
        this.hiveJob = Input.empty();
        this.labels = Input.empty();
        this.pigJob = Input.empty();
        this.prerequisiteStepIds = Input.empty();
        this.prestoJob = Input.empty();
        this.pysparkJob = Input.empty();
        this.scheduling = Input.empty();
        this.sparkJob = Input.empty();
        this.sparkRJob = Input.empty();
        this.sparkSqlJob = Input.empty();
        this.stepId = Input.empty();
    }

    public static Builder builder() {
        return new Builder();
    }

    public static Builder builder(WorkflowTemplateJobGetArgs defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private @Nullable Input<WorkflowTemplateJobHadoopJobGetArgs> hadoopJob;
        private @Nullable Input<WorkflowTemplateJobHiveJobGetArgs> hiveJob;
        private @Nullable Input<Map<String,String>> labels;
        private @Nullable Input<WorkflowTemplateJobPigJobGetArgs> pigJob;
        private @Nullable Input<List<String>> prerequisiteStepIds;
        private @Nullable Input<WorkflowTemplateJobPrestoJobGetArgs> prestoJob;
        private @Nullable Input<WorkflowTemplateJobPysparkJobGetArgs> pysparkJob;
        private @Nullable Input<WorkflowTemplateJobSchedulingGetArgs> scheduling;
        private @Nullable Input<WorkflowTemplateJobSparkJobGetArgs> sparkJob;
        private @Nullable Input<WorkflowTemplateJobSparkRJobGetArgs> sparkRJob;
        private @Nullable Input<WorkflowTemplateJobSparkSqlJobGetArgs> sparkSqlJob;
        private Input<String> stepId;

        public Builder() {
    	      // Empty
        }

        public Builder(WorkflowTemplateJobGetArgs defaults) {
    	      Objects.requireNonNull(defaults);
    	      this.hadoopJob = defaults.hadoopJob;
    	      this.hiveJob = defaults.hiveJob;
    	      this.labels = defaults.labels;
    	      this.pigJob = defaults.pigJob;
    	      this.prerequisiteStepIds = defaults.prerequisiteStepIds;
    	      this.prestoJob = defaults.prestoJob;
    	      this.pysparkJob = defaults.pysparkJob;
    	      this.scheduling = defaults.scheduling;
    	      this.sparkJob = defaults.sparkJob;
    	      this.sparkRJob = defaults.sparkRJob;
    	      this.sparkSqlJob = defaults.sparkSqlJob;
    	      this.stepId = defaults.stepId;
        }

        public Builder setHadoopJob(@Nullable Input<WorkflowTemplateJobHadoopJobGetArgs> hadoopJob) {
            this.hadoopJob = hadoopJob;
            return this;
        }

        public Builder setHadoopJob(@Nullable WorkflowTemplateJobHadoopJobGetArgs hadoopJob) {
            this.hadoopJob = Input.ofNullable(hadoopJob);
            return this;
        }

        public Builder setHiveJob(@Nullable Input<WorkflowTemplateJobHiveJobGetArgs> hiveJob) {
            this.hiveJob = hiveJob;
            return this;
        }

        public Builder setHiveJob(@Nullable WorkflowTemplateJobHiveJobGetArgs hiveJob) {
            this.hiveJob = Input.ofNullable(hiveJob);
            return this;
        }

        public Builder setLabels(@Nullable Input<Map<String,String>> labels) {
            this.labels = labels;
            return this;
        }

        public Builder setLabels(@Nullable Map<String,String> labels) {
            this.labels = Input.ofNullable(labels);
            return this;
        }

        public Builder setPigJob(@Nullable Input<WorkflowTemplateJobPigJobGetArgs> pigJob) {
            this.pigJob = pigJob;
            return this;
        }

        public Builder setPigJob(@Nullable WorkflowTemplateJobPigJobGetArgs pigJob) {
            this.pigJob = Input.ofNullable(pigJob);
            return this;
        }

        public Builder setPrerequisiteStepIds(@Nullable Input<List<String>> prerequisiteStepIds) {
            this.prerequisiteStepIds = prerequisiteStepIds;
            return this;
        }

        public Builder setPrerequisiteStepIds(@Nullable List<String> prerequisiteStepIds) {
            this.prerequisiteStepIds = Input.ofNullable(prerequisiteStepIds);
            return this;
        }

        public Builder setPrestoJob(@Nullable Input<WorkflowTemplateJobPrestoJobGetArgs> prestoJob) {
            this.prestoJob = prestoJob;
            return this;
        }

        public Builder setPrestoJob(@Nullable WorkflowTemplateJobPrestoJobGetArgs prestoJob) {
            this.prestoJob = Input.ofNullable(prestoJob);
            return this;
        }

        public Builder setPysparkJob(@Nullable Input<WorkflowTemplateJobPysparkJobGetArgs> pysparkJob) {
            this.pysparkJob = pysparkJob;
            return this;
        }

        public Builder setPysparkJob(@Nullable WorkflowTemplateJobPysparkJobGetArgs pysparkJob) {
            this.pysparkJob = Input.ofNullable(pysparkJob);
            return this;
        }

        public Builder setScheduling(@Nullable Input<WorkflowTemplateJobSchedulingGetArgs> scheduling) {
            this.scheduling = scheduling;
            return this;
        }

        public Builder setScheduling(@Nullable WorkflowTemplateJobSchedulingGetArgs scheduling) {
            this.scheduling = Input.ofNullable(scheduling);
            return this;
        }

        public Builder setSparkJob(@Nullable Input<WorkflowTemplateJobSparkJobGetArgs> sparkJob) {
            this.sparkJob = sparkJob;
            return this;
        }

        public Builder setSparkJob(@Nullable WorkflowTemplateJobSparkJobGetArgs sparkJob) {
            this.sparkJob = Input.ofNullable(sparkJob);
            return this;
        }

        public Builder setSparkRJob(@Nullable Input<WorkflowTemplateJobSparkRJobGetArgs> sparkRJob) {
            this.sparkRJob = sparkRJob;
            return this;
        }

        public Builder setSparkRJob(@Nullable WorkflowTemplateJobSparkRJobGetArgs sparkRJob) {
            this.sparkRJob = Input.ofNullable(sparkRJob);
            return this;
        }

        public Builder setSparkSqlJob(@Nullable Input<WorkflowTemplateJobSparkSqlJobGetArgs> sparkSqlJob) {
            this.sparkSqlJob = sparkSqlJob;
            return this;
        }

        public Builder setSparkSqlJob(@Nullable WorkflowTemplateJobSparkSqlJobGetArgs sparkSqlJob) {
            this.sparkSqlJob = Input.ofNullable(sparkSqlJob);
            return this;
        }

        public Builder setStepId(Input<String> stepId) {
            this.stepId = Objects.requireNonNull(stepId);
            return this;
        }

        public Builder setStepId(String stepId) {
            this.stepId = Input.of(Objects.requireNonNull(stepId));
            return this;
        }
        public WorkflowTemplateJobGetArgs build() {
            return new WorkflowTemplateJobGetArgs(hadoopJob, hiveJob, labels, pigJob, prerequisiteStepIds, prestoJob, pysparkJob, scheduling, sparkJob, sparkRJob, sparkSqlJob, stepId);
        }
    }
}
