// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package io.pulumi.gcp.dataproc;

import io.pulumi.core.Output;
import io.pulumi.core.annotations.Export;
import io.pulumi.core.annotations.ResourceType;
import io.pulumi.gcp.Utilities;
import io.pulumi.gcp.dataproc.JobArgs;
import io.pulumi.gcp.dataproc.inputs.JobState;
import io.pulumi.gcp.dataproc.outputs.JobHadoopConfig;
import io.pulumi.gcp.dataproc.outputs.JobHiveConfig;
import io.pulumi.gcp.dataproc.outputs.JobPigConfig;
import io.pulumi.gcp.dataproc.outputs.JobPlacement;
import io.pulumi.gcp.dataproc.outputs.JobPysparkConfig;
import io.pulumi.gcp.dataproc.outputs.JobReference;
import io.pulumi.gcp.dataproc.outputs.JobScheduling;
import io.pulumi.gcp.dataproc.outputs.JobSparkConfig;
import io.pulumi.gcp.dataproc.outputs.JobSparksqlConfig;
import io.pulumi.gcp.dataproc.outputs.JobStatus;
import java.lang.Boolean;
import java.lang.String;
import java.util.List;
import java.util.Map;
import javax.annotation.Nullable;

/**
 * Manages a job resource within a Dataproc cluster within GCE. For more information see
 * [the official dataproc documentation](https://cloud.google.com/dataproc/).
 * 
 * !> **Note:** This resource does not support 'update' and changing any attributes will cause the resource to be recreated.
 * 
 * {{% examples %}}
 * ## Example Usage
 * {{% example %}}
 * 
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as gcp from "@pulumi/gcp";
 * 
 * const mycluster = new gcp.dataproc.Cluster("mycluster", {region: "us-central1"});
 * // Submit an example spark job to a dataproc cluster
 * const spark = new gcp.dataproc.Job("spark", {
 *     region: mycluster.region,
 *     forceDelete: true,
 *     placement: {
 *         clusterName: mycluster.name,
 *     },
 *     sparkConfig: {
 *         mainClass: "org.apache.spark.examples.SparkPi",
 *         jarFileUris: ["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *         args: ["1000"],
 *         properties: {
 *             "spark.logConf": "true",
 *         },
 *         loggingConfig: {
 *             driverLogLevels: {
 *                 root: "INFO",
 *             },
 *         },
 *     },
 * });
 * // Submit an example pyspark job to a dataproc cluster
 * const pyspark = new gcp.dataproc.Job("pyspark", {
 *     region: mycluster.region,
 *     forceDelete: true,
 *     placement: {
 *         clusterName: mycluster.name,
 *     },
 *     pysparkConfig: {
 *         mainPythonFileUri: "gs://dataproc-examples-2f10d78d114f6aaec76462e3c310f31f/src/pyspark/hello-world/hello-world.py",
 *         properties: {
 *             "spark.logConf": "true",
 *         },
 *     },
 * });
 * export const sparkStatus = spark.statuses.apply(statuses => statuses[0].state);
 * export const pysparkStatus = pyspark.statuses.apply(statuses => statuses[0].state);
 * ```
 * ```python
 * import pulumi
 * import pulumi_gcp as gcp
 * 
 * mycluster = gcp.dataproc.Cluster("mycluster", region="us-central1")
 * # Submit an example spark job to a dataproc cluster
 * spark = gcp.dataproc.Job("spark",
 *     region=mycluster.region,
 *     force_delete=True,
 *     placement=gcp.dataproc.JobPlacementArgs(
 *         cluster_name=mycluster.name,
 *     ),
 *     spark_config=gcp.dataproc.JobSparkConfigArgs(
 *         main_class="org.apache.spark.examples.SparkPi",
 *         jar_file_uris=["file:///usr/lib/spark/examples/jars/spark-examples.jar"],
 *         args=["1000"],
 *         properties={
 *             "spark.logConf": "true",
 *         },
 *         logging_config=gcp.dataproc.JobSparkConfigLoggingConfigArgs(
 *             driver_log_levels={
 *                 "root": "INFO",
 *             },
 *         ),
 *     ))
 * # Submit an example pyspark job to a dataproc cluster
 * pyspark = gcp.dataproc.Job("pyspark",
 *     region=mycluster.region,
 *     force_delete=True,
 *     placement=gcp.dataproc.JobPlacementArgs(
 *         cluster_name=mycluster.name,
 *     ),
 *     pyspark_config=gcp.dataproc.JobPysparkConfigArgs(
 *         main_python_file_uri="gs://dataproc-examples-2f10d78d114f6aaec76462e3c310f31f/src/pyspark/hello-world/hello-world.py",
 *         properties={
 *             "spark.logConf": "true",
 *         },
 *     ))
 * pulumi.export("sparkStatus", spark.statuses[0].state)
 * pulumi.export("pysparkStatus", pyspark.statuses[0].state)
 * ```
 * ```csharp
 * using Pulumi;
 * using Gcp = Pulumi.Gcp;
 * 
 * class MyStack : Stack
 * {
 *     public MyStack()
 *     {
 *         var mycluster = new Gcp.Dataproc.Cluster("mycluster", new Gcp.Dataproc.ClusterArgs
 *         {
 *             Region = "us-central1",
 *         });
 *         // Submit an example spark job to a dataproc cluster
 *         var spark = new Gcp.Dataproc.Job("spark", new Gcp.Dataproc.JobArgs
 *         {
 *             Region = mycluster.Region,
 *             ForceDelete = true,
 *             Placement = new Gcp.Dataproc.Inputs.JobPlacementArgs
 *             {
 *                 ClusterName = mycluster.Name,
 *             },
 *             SparkConfig = new Gcp.Dataproc.Inputs.JobSparkConfigArgs
 *             {
 *                 MainClass = "org.apache.spark.examples.SparkPi",
 *                 JarFileUris = 
 *                 {
 *                     "file:///usr/lib/spark/examples/jars/spark-examples.jar",
 *                 },
 *                 Args = 
 *                 {
 *                     "1000",
 *                 },
 *                 Properties = 
 *                 {
 *                     { "spark.logConf", "true" },
 *                 },
 *                 LoggingConfig = new Gcp.Dataproc.Inputs.JobSparkConfigLoggingConfigArgs
 *                 {
 *                     DriverLogLevels = 
 *                     {
 *                         { "root", "INFO" },
 *                     },
 *                 },
 *             },
 *         });
 *         // Submit an example pyspark job to a dataproc cluster
 *         var pyspark = new Gcp.Dataproc.Job("pyspark", new Gcp.Dataproc.JobArgs
 *         {
 *             Region = mycluster.Region,
 *             ForceDelete = true,
 *             Placement = new Gcp.Dataproc.Inputs.JobPlacementArgs
 *             {
 *                 ClusterName = mycluster.Name,
 *             },
 *             PysparkConfig = new Gcp.Dataproc.Inputs.JobPysparkConfigArgs
 *             {
 *                 MainPythonFileUri = "gs://dataproc-examples-2f10d78d114f6aaec76462e3c310f31f/src/pyspark/hello-world/hello-world.py",
 *                 Properties = 
 *                 {
 *                     { "spark.logConf", "true" },
 *                 },
 *             },
 *         });
 *         this.SparkStatus = spark.Statuses.Apply(statuses => statuses[0].State);
 *         this.PysparkStatus = pyspark.Statuses.Apply(statuses => statuses[0].State);
 *     }
 * 
 *     [Output("sparkStatus")]
 *     public Output<string> SparkStatus { get; set; }
 *     [Output("pysparkStatus")]
 *     public Output<string> PysparkStatus { get; set; }
 * }
 * ```
 * ```go
 * package main
 * 
 * import (
 * 	"github.com/pulumi/pulumi-gcp/sdk/v6/go/gcp/dataproc"
 * 	"github.com/pulumi/pulumi/sdk/v3/go/pulumi"
 * )
 * 
 * func main() {
 * 	pulumi.Run(func(ctx *pulumi.Context) error {
 * 		mycluster, err := dataproc.NewCluster(ctx, "mycluster", &dataproc.ClusterArgs{
 * 			Region: pulumi.String("us-central1"),
 * 		})
 * 		if err != nil {
 * 			return err
 * 		}
 * 		spark, err := dataproc.NewJob(ctx, "spark", &dataproc.JobArgs{
 * 			Region:      mycluster.Region,
 * 			ForceDelete: pulumi.Bool(true),
 * 			Placement: &dataproc.JobPlacementArgs{
 * 				ClusterName: mycluster.Name,
 * 			},
 * 			SparkConfig: &dataproc.JobSparkConfigArgs{
 * 				MainClass: pulumi.String("org.apache.spark.examples.SparkPi"),
 * 				JarFileUris: pulumi.StringArray{
 * 					pulumi.String("file:///usr/lib/spark/examples/jars/spark-examples.jar"),
 * 				},
 * 				Args: pulumi.StringArray{
 * 					pulumi.String("1000"),
 * 				},
 * 				Properties: pulumi.StringMap{
 * 					"spark.logConf": pulumi.String("true"),
 * 				},
 * 				LoggingConfig: &dataproc.JobSparkConfigLoggingConfigArgs{
 * 					DriverLogLevels: pulumi.StringMap{
 * 						"root": pulumi.String("INFO"),
 * 					},
 * 				},
 * 			},
 * 		})
 * 		if err != nil {
 * 			return err
 * 		}
 * 		pyspark, err := dataproc.NewJob(ctx, "pyspark", &dataproc.JobArgs{
 * 			Region:      mycluster.Region,
 * 			ForceDelete: pulumi.Bool(true),
 * 			Placement: &dataproc.JobPlacementArgs{
 * 				ClusterName: mycluster.Name,
 * 			},
 * 			PysparkConfig: &dataproc.JobPysparkConfigArgs{
 * 				MainPythonFileUri: pulumi.String("gs://dataproc-examples-2f10d78d114f6aaec76462e3c310f31f/src/pyspark/hello-world/hello-world.py"),
 * 				Properties: pulumi.StringMap{
 * 					"spark.logConf": pulumi.String("true"),
 * 				},
 * 			},
 * 		})
 * 		if err != nil {
 * 			return err
 * 		}
 * 		ctx.Export("sparkStatus", spark.Statuses.ApplyT(func(statuses []dataproc.JobStatus) (string, error) {
 * 			return statuses[0].State, nil
 * 		}).(pulumi.StringOutput))
 * 		ctx.Export("pysparkStatus", pyspark.Statuses.ApplyT(func(statuses []dataproc.JobStatus) (string, error) {
 * 			return statuses[0].State, nil
 * 		}).(pulumi.StringOutput))
 * 		return nil
 * 	})
 * }
 * ```
 * {{% /example %}}
 * {{% /examples %}}
 * 
 * ## Import
 * 
 * This resource does not support import. 
 */
@ResourceType(type="gcp:dataproc/job:Job")
public class Job extends io.pulumi.resources.CustomResource {
    /**
     * If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
     * 
     */
    @Export(name="driverControlsFilesUri", type=String.class, parameters={})
    private Output<String> driverControlsFilesUri;

    /**
     * @return If present, the location of miscellaneous control files which may be used as part of job setup and handling. If not present, control files may be placed in the same location as driver_output_uri.
     * 
     */
    public Output<String> getDriverControlsFilesUri() {
        return this.driverControlsFilesUri;
    }
    /**
     * A URI pointing to the location of the stdout of the job's driver program.
     * 
     */
    @Export(name="driverOutputResourceUri", type=String.class, parameters={})
    private Output<String> driverOutputResourceUri;

    /**
     * @return A URI pointing to the location of the stdout of the job's driver program.
     * 
     */
    public Output<String> getDriverOutputResourceUri() {
        return this.driverOutputResourceUri;
    }
    /**
     * By default, you can only delete inactive jobs within
     * Dataproc. Setting this to true, and calling destroy, will ensure that the
     * job is first cancelled before issuing the delete.
     * 
     */
    @Export(name="forceDelete", type=Boolean.class, parameters={})
    private Output</* @Nullable */ Boolean> forceDelete;

    /**
     * @return By default, you can only delete inactive jobs within
     * Dataproc. Setting this to true, and calling destroy, will ensure that the
     * job is first cancelled before issuing the delete.
     * 
     */
    public Output</* @Nullable */ Boolean> getForceDelete() {
        return this.forceDelete;
    }
    /**
     * The config of Hadoop job
     * 
     */
    @Export(name="hadoopConfig", type=JobHadoopConfig.class, parameters={})
    private Output</* @Nullable */ JobHadoopConfig> hadoopConfig;

    /**
     * @return The config of Hadoop job
     * 
     */
    public Output</* @Nullable */ JobHadoopConfig> getHadoopConfig() {
        return this.hadoopConfig;
    }
    /**
     * The config of hive job
     * 
     */
    @Export(name="hiveConfig", type=JobHiveConfig.class, parameters={})
    private Output</* @Nullable */ JobHiveConfig> hiveConfig;

    /**
     * @return The config of hive job
     * 
     */
    public Output</* @Nullable */ JobHiveConfig> getHiveConfig() {
        return this.hiveConfig;
    }
    /**
     * The list of labels (key/value pairs) to add to the job.
     * 
     */
    @Export(name="labels", type=Map.class, parameters={String.class, String.class})
    private Output</* @Nullable */ Map<String,String>> labels;

    /**
     * @return The list of labels (key/value pairs) to add to the job.
     * 
     */
    public Output</* @Nullable */ Map<String,String>> getLabels() {
        return this.labels;
    }
    /**
     * The config of pag job.
     * 
     */
    @Export(name="pigConfig", type=JobPigConfig.class, parameters={})
    private Output</* @Nullable */ JobPigConfig> pigConfig;

    /**
     * @return The config of pag job.
     * 
     */
    public Output</* @Nullable */ JobPigConfig> getPigConfig() {
        return this.pigConfig;
    }
    /**
     * The config of job placement.
     * 
     */
    @Export(name="placement", type=JobPlacement.class, parameters={})
    private Output<JobPlacement> placement;

    /**
     * @return The config of job placement.
     * 
     */
    public Output<JobPlacement> getPlacement() {
        return this.placement;
    }
    /**
     * The project in which the `cluster` can be found and jobs
     * subsequently run against. If it is not provided, the provider project is used.
     * 
     */
    @Export(name="project", type=String.class, parameters={})
    private Output<String> project;

    /**
     * @return The project in which the `cluster` can be found and jobs
     * subsequently run against. If it is not provided, the provider project is used.
     * 
     */
    public Output<String> getProject() {
        return this.project;
    }
    /**
     * The config of pySpark job.
     * 
     */
    @Export(name="pysparkConfig", type=JobPysparkConfig.class, parameters={})
    private Output</* @Nullable */ JobPysparkConfig> pysparkConfig;

    /**
     * @return The config of pySpark job.
     * 
     */
    public Output</* @Nullable */ JobPysparkConfig> getPysparkConfig() {
        return this.pysparkConfig;
    }
    /**
     * The reference of the job
     * 
     */
    @Export(name="reference", type=JobReference.class, parameters={})
    private Output<JobReference> reference;

    /**
     * @return The reference of the job
     * 
     */
    public Output<JobReference> getReference() {
        return this.reference;
    }
    /**
     * The Cloud Dataproc region. This essentially determines which clusters are available
     * for this job to be submitted to. If not specified, defaults to `global`.
     * 
     */
    @Export(name="region", type=String.class, parameters={})
    private Output</* @Nullable */ String> region;

    /**
     * @return The Cloud Dataproc region. This essentially determines which clusters are available
     * for this job to be submitted to. If not specified, defaults to `global`.
     * 
     */
    public Output</* @Nullable */ String> getRegion() {
        return this.region;
    }
    /**
     * Optional. Job scheduling configuration.
     * 
     */
    @Export(name="scheduling", type=JobScheduling.class, parameters={})
    private Output</* @Nullable */ JobScheduling> scheduling;

    /**
     * @return Optional. Job scheduling configuration.
     * 
     */
    public Output</* @Nullable */ JobScheduling> getScheduling() {
        return this.scheduling;
    }
    /**
     * The config of the Spark job.
     * 
     */
    @Export(name="sparkConfig", type=JobSparkConfig.class, parameters={})
    private Output</* @Nullable */ JobSparkConfig> sparkConfig;

    /**
     * @return The config of the Spark job.
     * 
     */
    public Output</* @Nullable */ JobSparkConfig> getSparkConfig() {
        return this.sparkConfig;
    }
    /**
     * The config of SparkSql job
     * 
     */
    @Export(name="sparksqlConfig", type=JobSparksqlConfig.class, parameters={})
    private Output</* @Nullable */ JobSparksqlConfig> sparksqlConfig;

    /**
     * @return The config of SparkSql job
     * 
     */
    public Output</* @Nullable */ JobSparksqlConfig> getSparksqlConfig() {
        return this.sparksqlConfig;
    }
    /**
     * The status of the job.
     * 
     */
    @Export(name="statuses", type=List.class, parameters={JobStatus.class})
    private Output<List<JobStatus>> statuses;

    /**
     * @return The status of the job.
     * 
     */
    public Output<List<JobStatus>> getStatuses() {
        return this.statuses;
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public Job(String name) {
        this(name, JobArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public Job(String name, JobArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public Job(String name, JobArgs args, @Nullable io.pulumi.resources.CustomResourceOptions options) {
        super("gcp:dataproc/job:Job", name, args == null ? JobArgs.Empty : args, makeResourceOptions(options, Output.empty()));
    }

    private Job(String name, Output<String> id, @Nullable JobState state, @Nullable io.pulumi.resources.CustomResourceOptions options) {
        super("gcp:dataproc/job:Job", name, state, makeResourceOptions(options, id));
    }

    private static io.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable io.pulumi.resources.CustomResourceOptions options, @Nullable Output<String> id) {
        var defaultOptions = io.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return io.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static Job get(String name, Output<String> id, @Nullable JobState state, @Nullable io.pulumi.resources.CustomResourceOptions options) {
        return new Job(name, id, state, options);
    }
}
