// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package io.pulumi.gcp.dataproc;

import io.pulumi.core.Input;
import io.pulumi.core.Output;
import io.pulumi.core.internal.annotations.OutputExport;
import io.pulumi.core.internal.annotations.ResourceType;
import io.pulumi.gcp.Utilities;
import io.pulumi.gcp.dataproc.JobArgs;
import io.pulumi.gcp.dataproc.inputs.JobState;
import io.pulumi.gcp.dataproc.outputs.JobHadoopConfig;
import io.pulumi.gcp.dataproc.outputs.JobHiveConfig;
import io.pulumi.gcp.dataproc.outputs.JobPigConfig;
import io.pulumi.gcp.dataproc.outputs.JobPlacement;
import io.pulumi.gcp.dataproc.outputs.JobPysparkConfig;
import io.pulumi.gcp.dataproc.outputs.JobReference;
import io.pulumi.gcp.dataproc.outputs.JobScheduling;
import io.pulumi.gcp.dataproc.outputs.JobSparkConfig;
import io.pulumi.gcp.dataproc.outputs.JobSparksqlConfig;
import io.pulumi.gcp.dataproc.outputs.JobStatus;
import java.lang.Boolean;
import java.lang.String;
import java.util.List;
import java.util.Map;
import javax.annotation.Nullable;

@ResourceType(type="gcp:dataproc/job:Job")
public class Job extends io.pulumi.resources.CustomResource {
    @OutputExport(name="driverControlsFilesUri", type=String.class, parameters={})
    private Output<String> driverControlsFilesUri;

    public Output<String> getDriverControlsFilesUri() {
        return this.driverControlsFilesUri;
    }
    @OutputExport(name="driverOutputResourceUri", type=String.class, parameters={})
    private Output<String> driverOutputResourceUri;

    public Output<String> getDriverOutputResourceUri() {
        return this.driverOutputResourceUri;
    }
    @OutputExport(name="forceDelete", type=Boolean.class, parameters={})
    private Output</* @Nullable */ Boolean> forceDelete;

    public Output</* @Nullable */ Boolean> getForceDelete() {
        return this.forceDelete;
    }
    @OutputExport(name="hadoopConfig", type=JobHadoopConfig.class, parameters={})
    private Output</* @Nullable */ JobHadoopConfig> hadoopConfig;

    public Output</* @Nullable */ JobHadoopConfig> getHadoopConfig() {
        return this.hadoopConfig;
    }
    @OutputExport(name="hiveConfig", type=JobHiveConfig.class, parameters={})
    private Output</* @Nullable */ JobHiveConfig> hiveConfig;

    public Output</* @Nullable */ JobHiveConfig> getHiveConfig() {
        return this.hiveConfig;
    }
    @OutputExport(name="labels", type=Map.class, parameters={String.class, String.class})
    private Output</* @Nullable */ Map<String,String>> labels;

    public Output</* @Nullable */ Map<String,String>> getLabels() {
        return this.labels;
    }
    @OutputExport(name="pigConfig", type=JobPigConfig.class, parameters={})
    private Output</* @Nullable */ JobPigConfig> pigConfig;

    public Output</* @Nullable */ JobPigConfig> getPigConfig() {
        return this.pigConfig;
    }
    @OutputExport(name="placement", type=JobPlacement.class, parameters={})
    private Output<JobPlacement> placement;

    public Output<JobPlacement> getPlacement() {
        return this.placement;
    }
    @OutputExport(name="project", type=String.class, parameters={})
    private Output<String> project;

    public Output<String> getProject() {
        return this.project;
    }
    @OutputExport(name="pysparkConfig", type=JobPysparkConfig.class, parameters={})
    private Output</* @Nullable */ JobPysparkConfig> pysparkConfig;

    public Output</* @Nullable */ JobPysparkConfig> getPysparkConfig() {
        return this.pysparkConfig;
    }
    @OutputExport(name="reference", type=JobReference.class, parameters={})
    private Output<JobReference> reference;

    public Output<JobReference> getReference() {
        return this.reference;
    }
    @OutputExport(name="region", type=String.class, parameters={})
    private Output</* @Nullable */ String> region;

    public Output</* @Nullable */ String> getRegion() {
        return this.region;
    }
    @OutputExport(name="scheduling", type=JobScheduling.class, parameters={})
    private Output</* @Nullable */ JobScheduling> scheduling;

    public Output</* @Nullable */ JobScheduling> getScheduling() {
        return this.scheduling;
    }
    @OutputExport(name="sparkConfig", type=JobSparkConfig.class, parameters={})
    private Output</* @Nullable */ JobSparkConfig> sparkConfig;

    public Output</* @Nullable */ JobSparkConfig> getSparkConfig() {
        return this.sparkConfig;
    }
    @OutputExport(name="sparksqlConfig", type=JobSparksqlConfig.class, parameters={})
    private Output</* @Nullable */ JobSparksqlConfig> sparksqlConfig;

    public Output</* @Nullable */ JobSparksqlConfig> getSparksqlConfig() {
        return this.sparksqlConfig;
    }
    @OutputExport(name="statuses", type=List.class, parameters={JobStatus.class})
    private Output<List<JobStatus>> statuses;

    public Output<List<JobStatus>> getStatuses() {
        return this.statuses;
    }

    public Job(String name, JobArgs args, @Nullable io.pulumi.resources.CustomResourceOptions options) {
        super("gcp:dataproc/job:Job", name, args == null ? JobArgs.Empty : args, makeResourceOptions(options, Input.empty()));
    }

    private Job(String name, Input<String> id, @Nullable JobState state, @Nullable io.pulumi.resources.CustomResourceOptions options) {
        super("gcp:dataproc/job:Job", name, state, makeResourceOptions(options, id));
    }

    private static io.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable io.pulumi.resources.CustomResourceOptions options, @Nullable Input<String> id) {
        var defaultOptions = io.pulumi.resources.CustomResourceOptions.builder()
            .setVersion(Utilities.getVersion())
            .build();
        return io.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    public static Job get(String name, Input<String> id, @Nullable JobState state, @Nullable io.pulumi.resources.CustomResourceOptions options) {
        return new Job(name, id, state, options);
    }
}
